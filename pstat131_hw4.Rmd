---
title: "Pstat 131, Homework 4"
author: "Marcus Eriksson"
date: "Section W 5pm"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, 
                      cache=TRUE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center')
indent1 = '    '      
indent2 = paste(rep(indent1, 2), collapse='')
```

```{r}
#install.packages("randomForest")
#install.packages("gbm")
#install.packages("e1071")
#install.packages("imager")

library(tidyverse)
library(tree)
library(randomForest)
library(gbm)
library(ROCR)
library(e1071)
library(imager)
```

####1) Fundamentals of the bootstrap:


####a) Sample of size n, probability of any observation j is not in a bootstrap sample:

P(j not in bootstrap sample) = ((1-(1/n))^n)

####b) Compute probability for n = 1000

```{r}
n = 1000

p_not_boot <- ((1-(1/n))^n)
p_not_boot

```


####c) Verify that the calculations are reasonable by resampling and print number of missing observations

```{r}
set.seed(1)
boot_vector = NULL

for(i in 1:1000){
  boot_vector = sample(1:1000, replace =TRUE)
  
}
num = length(unique(boot_vector))
num

(1-num/1000)

```

It seems like our calculations are reasonable since from part b) we determined that the probability of j not appearing in the bootstrap sample was about .367 and when performing resampling we got that 37.8% of observations were not in the bootstrao sample. 

####d)

```{r}
set.seed(1)
baskets = NULL

for(i in 1:62){
  baskets[i]=1

}
for(i in 63:126){
  baskets[i]=0
}

bsFG = NULL
for(i in 1:1000){
  basket_boot = NULL
  for(j in 1:126){
    basket_boot = sample(baskets, i , replace = TRUE)
  }
  bsFG = append(bsFG, mean(basket_boot))
}

hist(bsFG)

quantile(bsFG, probs = seq(0,1,.025))[2]
quantile(bsFG, probs = seq(0,1,.025))[40]


```

From the bootstrap sample we obtain a confidence interval of (0.4337727, 0.5610421) for Curry's end of season field goal percentage. From the regression to the mean concept and law of large numbers we would expect his field goal percentage to decrease over time to his true shooting "skill".




###2) Eigenfaces:

```{r}
load("faces_array.RData")

face_mat <- sapply(1:1000, function(i) as.numeric(faces_array[, , i])) %>% t
plot_face <- function(image_vector) {
plot(as.cimg(t(matrix(image_vector, ncol=100))), axes=FALSE, asp=1)
}


```



####a) Find the average face and plot:

```{r}

average_face <- apply(face_mat, 2, mean) 

plot_face(average_face)
```


####b) Running PCA on face_mat, 

```{r}
face_pca <- prcomp(face_mat, center = TRUE, scale  = FALSE)

std_dev <- face_pca$sdev

var_face <- std_dev^2

pve=var_face/sum(var_face)
#pve

#pve plot
plot(pve, xlab="Principal Component",
ylab="Proportion of Variance Explained ", ylim=c(0,.3),type='b')

#cumulative pve plot

plot(cumsum(pve), xlab="Principal Component ",
ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')

#cumsum(pve)

```

When examining the cumulative pve; we find that we require at least 5 principle components to explain 50% of the total variation in the face images.

####c) Plot the first 16 principle component directions

```{r}
par(mfrow = c(4,4))

for(i in 1:16){
  plot_face(face_pca$rotation[,i])
}


```

From the first 16 principle components we see the basic elements of faces. The first few PCs are mostly just basic head shapes with no real details on them. On the later images it becomes easier to see defined characteristics such as eyes, a nose and a mouth.

####d) Examining the faces with the highest and the smallest values for specified PCs

```{r}
pc1 <- face_pca$x[,1]

pc1 <- order(pc1, decreasing = TRUE)
head(pc1, 5)

for(i in 1:5){
  plot_face(face_mat[pc1[i],])
} 

pc1_len <- length(pc1)

for(i in 0:4){
  plot_face(face_mat[pc1[pc1_len - i],])
}
```

From analyzing the first component it seems like it is the best at capturing the variability in the overall shape of the faces and their contrast with the background. We see a variety of hairstyles in the faces examined here and facial structures.

####e) Repeat part d with PC5:

```{r}
pc5 <- face_pca$x[,5]

pc5 <- order(pc5, decreasing = TRUE)
head(pc5, 5)

for(i in 1:5){
  plot_face(face_mat[pc5[i],])
} 

pc5_len <- length(pc5)

for(i in 0:4){
  plot_face(face_mat[pc5[pc5_len - i],])
}
```

Once again we see that the principle component captures the variation in the contrast of the face with the background. Comparing the results of PC1 and PC5 it would seem like PC1 would be more useful for face recognition since it makes it more clear where the face "ends" and the background begins.




###3) Logistic regression with polynomial features:

####a) read in the file and plot data by class:

```{r}
library(ggplot2)
nonlinear <- read_csv("nonlinear.csv")
glimpse(nonlinear)

ggplot(aes(X1, X2), data = nonlinear)+geom_point(alpha = .5, aes(color = Y))
```

####b) fit a logistic regression model of Y on X1 and X2: 

```{r}




# grid of points over sample space
gr <- expand.grid(X1=seq(-5, 5, by=0.1), # sample points in X1
X2=seq(-5, 5, by=0.1)) # sample points in X2

non_lin = nonlinear[2:4]
nonlinear_glm <- glm(Y~., data = non_lin, family = binomial)

non_lin_predict <- predict(nonlinear_glm, gr, type = "response")
non_lin_ylab <- as.factor(ifelse(non_lin_predict <= .5, 0, 1)) 
ggplot(gr, aes(gr$X1, gr$X2))+geom_raster(aes(fill = non_lin_ylab), alpha = .05)+ geom_point(aes( non_lin$X1, non_lin$X2), data = non_lin)

 
```

####c) Fit a model involving a 2nd degree polynomial of X1 and X2 with interaction terms:

```{r}
non_lin2 <- glm(Y~ poly(X1,2, raw = T) + poly(X2, 2, raw =T) + X1*X2, family = binomial ,data = nonlinear)
summary(non_lin2)

non_lin2.pred  = predict(non_lin2, newdata = gr, type = "response")
gr$y = as.factor(ifelse(non_lin2.pred <= .5, 0 , 1))
ggplot(gr, aes(X1, X2)) + geom_raster(aes(fill = gr$y),data = gr ,alpha = .5)+geom_point(aes(color = Y), data = non_lin)
```


####d) Fit a logistic regression model with a 5th degree regression model:

```{r}
non_lin5 <- glm(Y~ poly(X1,5, raw = T) + poly(X2, 5, raw =T), family = binomial ,data = nonlinear)
summary(non_lin5)

non_lin5.pred  = predict(non_lin5, newdata = gr, type = "response")
gr$y = as.factor(ifelse(non_lin5.pred <= .5, 0 , 1))
ggplot(gr, aes(X1, X2)) + geom_raster(aes(fill = gr$y),data = gr ,alpha = .5)+geom_point(aes(color = Y), data = non_lin)

```

From the plot we see that values in the triangular region are being rejected.

####e) Comparing the polynomial models and linear model:

The 2nd degree polynomial model appears to have very high bias as it has an oval near the middle of the plot where values are rejected. The 5th degree polynomial model has the highest bias since it has a more condensed acceptance region. The linear model has the highest variance but lowest bias. 

####f) Create 3 bootstrap replicates of the original dataset, fit linear replicates and plot class predictions:

```{r}
set.seed(1)
par(mfrow = c(3,2))

#creat boostrap replicates
nonlinear_boot <- replicate(3, nonlinear[sample(1:nrow(nonlinear), replace = TRUE),])

for(i in 1:3){
  boot.df <- as.data.frame(nonlinear_boot[, i])
  boot.glm <- glm(Y~X1+X2, data = boot.df, family = binomial)


#get estimates

est_prob <- predict(boot.glm, newdata = gr, type = "response") 
gr <- gr %>% mutate(class.gr = as.factor(ifelse(est_prob <= .5, 0, 1)))


print(ggplot(data = gr, aes(X1, X2 ))+geom_raster(aes(fill = class.gr), alpha = .5)+ 
        geom_point(aes(color = Y), data = boot.df))

}


#5th order polynomial

for(i in 1:3){
  boot.df <- as.data.frame(nonlinear_boot[,i])
  boot5 <- glm(Y~poly(X1, 5, raw = TRUE)+poly(X2, 5, raw = TRUE),
               data = boot.df, family = binomial)
  #get estimates

est_prob <- predict(boot5, newdata = gr, type = "response") 
gr <- gr %>% mutate(class.gr = as.factor(ifelse(est_prob <= .5, 0, 1)))


print(ggplot(data = gr, aes(X1, X2 ))+geom_raster(aes(fill = class.gr), alpha = .5)+ 
        geom_point(aes(color = Y), data = boot.df))
  
}
```

From examining the plots it appears as if bias increases as the degree of the polynomial increases and variance decreases as the degree of the polynomial increases.

###4)

####a) Load Caravan data and split the data into the first 1000 observations and the rest of the observations as the test set:

```{r}
library(ISLR)
data("Caravan")
#Caravan

#training data
caravan_train <- Caravan[c(1:1000),]
#caravan_train

caravan_test <- Caravan[c(1001:5822),]
#caravan_test

```


####b) Fit a boosting model to the training set with Purchase as the response and other variables as the predictors:

```{r}
set.seed(1)
boost.caravan = gbm(ifelse(Purchase=="Yes",1,0)~., data=caravan_train,
distribution="bernoulli",shrinkage = .01)

summary(boost.caravan)
```

The most important variables appear to be PPERSAUT, MGODGE, MOPLHOOG and MKOOPKLA.


####c) Fit a random forest model to the training set:


```{r}
rf.caravan = randomForest(Purchase ~ ., data=caravan_train, importance=TRUE)
rf.caravan

varImpPlot(rf.caravan)
```

Out of bag estimate error: 6.2%
Variables sub-sampled at each split: 9
Number of trees used to fit the data: 500
It appears that the important variables obtained by boosting and the random forest method are similar but not identical. 


####d) Use both models to predict the response on the test data:

```{r}
caravan_test$P <- predict(rf.caravan, newdata = caravan_test, type = "prob")[,2]

caravan_test$yrf <- ifelse(caravan_test$P >= .2, 1, 0)

caravan_test$P <- predict(boost.caravan, newdata = caravan_test, n.trees = 500, type = "response")
caravan_test$yb <- ifelse(caravan_test$P >= .2, 1, 0 )

table(truth = caravan_test$Purchase, predicted =caravan_test$yb)
table(truth = caravan_test$Purchase, predicted = caravan_test$yrf)


```

In the random forests model about 81% of those predicted to make purchases actually made purchases.


###5) An SVMs prediction of drug use:

```{r}
drug_use <- read_csv('drug.csv',
col_names = c('ID','Age','Gender','Education','Country','Ethnicity',
'Nscore','Escore','Oscore','Ascore','Cscore','Impulsive',
'SS','Alcohol','Amphet','Amyl','Benzos','Caff','Cannabis',
'Choc','Coke','Crack','Ecstasy','Heroin','Ketamine','Legalh','LSD',
'Meth', 'Mushrooms', 'Nicotine', 'Semer','VSA'))

drug_use <- drug_use %>% mutate(recent_cannabis_use = factor(ifelse(Cannabis >= "CL3", "Yes", "No"), levels = c("No", "Yes")))
```

####a) Split the data into training and test data. Use a random sample of 1500 observations for the training data:



```{r}
set.seed(1)

sample_size = 1500
train_drug <- sample((1:nrow(drug_use)), size = sample_size)
drug.train <- drug_use[train_drug,]



test_drug <- drug_use[-train_drug,]
#test_drug

drug_subset <- drug_use %>% select(Age:SS, recent_cannabis_use)

svm_drug <- svm(recent_cannabis_use~., data = drug_subset, kernal = "radial", cost = 1, scale = FALSE)

pred_svm <- predict(svm_drug, test_drug, type = "class")

#confusion matrix:
table(truth = test_drug$recent_cannabis_use, predicted = pred_svm)
```


####b) Using the tune function to perform cross validation:

```{r}

tune_drug <- tune(svm, recent_cannabis_use ~ . , data = drug_subset[train_drug,], kernel = "radial", ranges = (cost=c(0.001, 0.01, 0.1,1,10,100)))
#drug_use$recent_cannabis_use


tune_drug$best.model

table(predict(tune_drug$best.model, newdata = drug_subset[-train_drug,]), truth = test_drug$recent_cannabis_use)
```


The optimal cost is 1 with a corresponding training error of about .008.








